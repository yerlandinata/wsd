{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "from preprocessor import (\n",
    "    clean_word, stemmer, pipe, remove_punctuation,\n",
    "    normalize_money, normalize_number, normalize_weekday,\n",
    "    normalize_month, create_stemmer, create_stop_words_remover\n",
    ")\n",
    "from functools import reduce\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_real_sentence = lambda s: (\n",
    "    len(s) > 10 and\n",
    "    len(s.split()) > 10 and\n",
    "    s[0] in string.ascii_letters  and \n",
    "    '|' not in s and \n",
    "    '=' not in s and \n",
    "    'html' not in s and \n",
    "    ':' not in s and\n",
    "    '/' not in s\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_words = '''asing atas badan baru berat \n",
    "besar bidang bintang bisa buah \n",
    "bulan bunga cabang cerah coklat \n",
    "dalam dasar dunia halaman harapan \n",
    "jalan jam jaringan kabur kaki \n",
    "kali kepala ketat kulit kunci \n",
    "layar lebat lingkungan mata membawa \n",
    "memecahkan menangkap mendorong menerima mengandung \n",
    "mengejar mengeluarkan mengikat mengisi menjaga \n",
    "menurunkan menyusun nilai panas pembagian \n",
    "rapat sarung tengah tinggi '''.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exception_words = [w for w in list(map(stemmer.stem, annotated_words)) if w not in annotated_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 38.5666457\n"
     ]
    }
   ],
   "source": [
    "begin = time.perf_counter()\n",
    "raw = open('../idwiki-latest-pages-articles-full.xml', 'r').readlines()\n",
    "print('elapsed time:', time.perf_counter() - begin)\n",
    "WIKI_LEN = len(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List and Clean Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_sentence = pipe(\n",
    "    remove_punctuation,\n",
    "    normalize_money,\n",
    "    normalize_number,\n",
    "    normalize_weekday,\n",
    "    normalize_month,\n",
    "    create_stemmer(annotated_words, exception_words),\n",
    "    create_stop_words_remover(annotated_words, exception_words),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw read: 21.32 % | Instances collected: 161221| Time elapsed: 1801 s | Time left: -1 ss\n",
      "Time's up, raw lines read:  15023138\n"
     ]
    }
   ],
   "source": [
    "clean_sentences = []\n",
    "begin = time.perf_counter()\n",
    "max_time = 1800\n",
    "\n",
    "for i in range(WIKI_LEN):\n",
    "    \n",
    "    sentences = []\n",
    "    for s in raw[i].split('.'):\n",
    "        sentences.append(s + '.')\n",
    "    for s in sentences:\n",
    "        if not is_real_sentence(s):\n",
    "            continue\n",
    "        s = s.replace('[[', '').replace(']]', '').replace('\\n', '').replace(\"''\", \"\").replace(\"'''\", \"\").replace('&quot;', '')\n",
    "        clean_sentences.append(preprocess_sentence(s))\n",
    "    if i % (WIKI_LEN//5000) == 0:\n",
    "        elapsed = int(time.perf_counter() - begin)\n",
    "        sys.stdout.write(\"\\rRaw read: {0:.2f} % | Instances collected: {1}| Time elapsed: {2} s | Time left: {3} s\".format(\n",
    "            i/WIKI_LEN*100, len(clean_sentences), elapsed, max_time - elapsed\n",
    "        ))\n",
    "        sys.stdout.flush()\n",
    "        if elapsed > max_time:\n",
    "            print()\n",
    "            print('Time\\'s up, raw lines read: ', i)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../wikipedia_clean_sentences.txt', 'w')\n",
    "for s in clean_sentences:\n",
    "    f.write(s + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Processed Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences = open('../wikipedia_clean_sentences.txt', 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00 | Time elapsed: 1 s"
     ]
    }
   ],
   "source": [
    "tokenized_clean_sentences = []\n",
    "begin = time.perf_counter()\n",
    "for i in range(len(clean_sentences)):\n",
    "    tokenized_clean_sentences.append(clean_sentences[i].split())  \n",
    "    if i % (1000) == 0:\n",
    "        sys.stdout.write(\"\\r{0:.2f} | Time elapsed: {1} s\".format(\n",
    "            i/len(clean_sentences), int(time.perf_counter() - begin)\n",
    "        ))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 27.277658900000006\n"
     ]
    }
   ],
   "source": [
    "begin = time.perf_counter()\n",
    "embedding_model = gensim.models.Word2Vec(tokenized_clean_sentences, window=5, size=EMBEDDING_SIZE, workers=7)\n",
    "print('elapsed time:', time.perf_counter() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.wv.save_word2vec_format('../wikipedia_indonesia_embedding50.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('revisi', 0.8082094788551331),\n",
       " ('kamis', 0.795890212059021),\n",
       " ('tanggal', 0.791890025138855),\n",
       " ('minggu', 0.7812029123306274),\n",
       " ('selasa', 0.776839017868042),\n",
       " ('bulan', 0.7645943760871887),\n",
       " ('gapeka', 0.7540700435638428),\n",
       " ('senin', 0.7530033588409424),\n",
       " ('sabtu', 0.7500444054603577),\n",
       " ('dibulan', 0.7448893785476685)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.wv.most_similar(positive='kompas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

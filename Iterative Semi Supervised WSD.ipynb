{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "import gensim\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "import time\n",
    "from functools import reduce\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from functools import reduce\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from preprocessor import normalize_money, normalize_number, stemmer, pipe\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MajorLabelClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        y = list(y)\n",
    "        self.possible_y_ = set(y)\n",
    "        self.total_example_ = len(y)\n",
    "        self.major_label_count_ = 0\n",
    "        for p in self.possible_y_:\n",
    "            count = y.count(p)\n",
    "            if count > self.major_label_count_:\n",
    "                self.major_label_ = p\n",
    "                self.major_label_count_ = count\n",
    "        self.fitted_ = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self.major_label_ for i in range(len(X))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>kata</th>\n",
       "      <th>sense</th>\n",
       "      <th>kalimat</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>clean</th>\n",
       "      <th>targetpos_clean</th>\n",
       "      <th>targetpos_ori</th>\n",
       "      <th>targetpos_pos_tag</th>\n",
       "      <th>verbs</th>\n",
       "      <th>nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cerah</td>\n",
       "      <td>4801</td>\n",
       "      <td>cuaca cerah adalah lazim panjang tahun</td>\n",
       "      <td>NN NN VB NN NN NN Z</td>\n",
       "      <td>cuaca cerah lazim</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>adalah</td>\n",
       "      <td>lazim panjang tahun cuaca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>cerah</td>\n",
       "      <td>4801</td>\n",
       "      <td>gambar yang hasil oleh layarnya cukup cerah da...</td>\n",
       "      <td>NNP SC VB IN NN RB JJ CC VB NN SC JJ VB NN SC ...</td>\n",
       "      <td>gambar hasil layarnya cerah milik speaker hasi...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>milik hasil hasil</td>\n",
       "      <td>speaker suara jernih layar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>cerah</td>\n",
       "      <td>4803</td>\n",
       "      <td>masa depan yang cerah bagi pemuda umur somenum...</td>\n",
       "      <td>NN NN SC VB IN NN NN CD IN NNP NNP CD Z</td>\n",
       "      <td>cerah bagi pemuda umur prancis abad</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>pemuda umur depan masa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>cerah</td>\n",
       "      <td>4801</td>\n",
       "      <td>cor caroli alpha canum venaticorum nama lengka...</td>\n",
       "      <td>NNP NNP Z NNP NNP NNP Z Z Z NN RB VB NNP NNP N...</td>\n",
       "      <td>cor caroli alpha canum venaticorum nama lengka...</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>adalah adalah</td>\n",
       "      <td>rasi bintang nama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>cerah</td>\n",
       "      <td>4801</td>\n",
       "      <td>sanders lebih suka cat air untuk lilo dengan m...</td>\n",
       "      <td>NN RB VB NN NN SC NNP IN NN VB NN NN NN NN NN Z</td>\n",
       "      <td>sanders suka cat air lilo maksud tampil warna ...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>tampil suka</td>\n",
       "      <td>buku cerita gambar warna maksud air cat sanders</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   kata sense                                            kalimat  \\\n",
       "0           0  cerah  4801             cuaca cerah adalah lazim panjang tahun   \n",
       "1           1  cerah  4801  gambar yang hasil oleh layarnya cukup cerah da...   \n",
       "2           2  cerah  4803  masa depan yang cerah bagi pemuda umur somenum...   \n",
       "3           3  cerah  4801  cor caroli alpha canum venaticorum nama lengka...   \n",
       "4           4  cerah  4801  sanders lebih suka cat air untuk lilo dengan m...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0                                NN NN VB NN NN NN Z   \n",
       "1  NNP SC VB IN NN RB JJ CC VB NN SC JJ VB NN SC ...   \n",
       "2            NN NN SC VB IN NN NN CD IN NNP NNP CD Z   \n",
       "3  NNP NNP Z NNP NNP NNP Z Z Z NN RB VB NNP NNP N...   \n",
       "4    NN RB VB NN NN SC NNP IN NN VB NN NN NN NN NN Z   \n",
       "\n",
       "                                               clean  targetpos_clean  \\\n",
       "0                                  cuaca cerah lazim                1   \n",
       "1  gambar hasil layarnya cerah milik speaker hasi...                3   \n",
       "2                cerah bagi pemuda umur prancis abad                0   \n",
       "3  cor caroli alpha canum venaticorum nama lengka...               12   \n",
       "4  sanders suka cat air lilo maksud tampil warna ...                8   \n",
       "\n",
       "   targetpos_ori  targetpos_pos_tag              verbs  \\\n",
       "0              1                  1             adalah   \n",
       "1              6                  6  milik hasil hasil   \n",
       "2              3                  3                      \n",
       "3             16                 21      adalah adalah   \n",
       "4             11                 11        tampil suka   \n",
       "\n",
       "                                             nouns  \n",
       "0                        lazim panjang tahun cuaca  \n",
       "1                       speaker suara jernih layar  \n",
       "2                           pemuda umur depan masa  \n",
       "3                                rasi bintang nama  \n",
       "4  buku cerita gambar warna maksud air cat sanders  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw = pd.read_csv('train_data.csv').fillna('')\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_word = set(train_raw.kata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RARE_LIMIT = 2\n",
    "sense_set = set(train_raw.sense)\n",
    "\n",
    "rare_sense = set(filter(lambda s: len(train_raw.query('sense == \"{}\"'.format(s))) <= RARE_LIMIT, sense_set))\n",
    "len(rare_sense)\n",
    "\n",
    "train_raw_kata = []\n",
    "train_raw_sense = []\n",
    "train_raw_kalimat = []\n",
    "train_raw_clean = []\n",
    "train_raw_pos_clean = []\n",
    "train_raw_pos_ori = []\n",
    "train_raw_pos_tags = []\n",
    "train_raw_pos_pos_tag = []\n",
    "train_raw_verbs = []\n",
    "train_raw_nouns = []\n",
    "\n",
    "for i in range(len(train_raw)):\n",
    "    row = train_raw.iloc[i]\n",
    "    if row.sense not in rare_sense:\n",
    "        train_raw_kata.append(row.kata)\n",
    "        train_raw_sense.append(row.sense)\n",
    "        train_raw_kalimat.append(row.kalimat)\n",
    "        train_raw_clean.append(row.clean)\n",
    "        train_raw_pos_clean.append(row.targetpos_clean)\n",
    "        train_raw_pos_ori.append(row.targetpos_ori)\n",
    "        train_raw_pos_tags.append(row.pos_tags)\n",
    "        train_raw_pos_pos_tag.append(row.targetpos_pos_tag)\n",
    "        train_raw_verbs.append(row.verbs)\n",
    "        train_raw_nouns.append(row.nouns)\n",
    "\n",
    "train_raw = pd.DataFrame({\n",
    "    'kata': train_raw_kata,\n",
    "    'sense': train_raw_sense,\n",
    "    'kalimat': train_raw_kalimat,\n",
    "    'clean': train_raw_clean,\n",
    "    'targetpos_clean': train_raw_pos_clean,\n",
    "    'targetpos_ori': train_raw_pos_ori,\n",
    "    'pos_tags': train_raw_pos_tags,\n",
    "    'targetpos_pos_tag': train_raw_pos_pos_tag,\n",
    "    'verbs': train_raw_verbs,\n",
    "    'nouns': train_raw_nouns,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rare_sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8379"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5301', '5302'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_raw.query('kata == \"asing\"').sense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>kata</th>\n",
       "      <th>kalimat</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>clean</th>\n",
       "      <th>targetpos_clean</th>\n",
       "      <th>targetpos_ori</th>\n",
       "      <th>targetpos_pos_tag</th>\n",
       "      <th>verbs</th>\n",
       "      <th>nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>asing</td>\n",
       "      <td>para cinta film indonesia atau tv pasti tak as...</td>\n",
       "      <td>DT NN NN NN CC NNP Z RB NEG JJ VB RB Z</td>\n",
       "      <td>cinta film indonesia tv asing dengar nama</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>dengar</td>\n",
       "      <td>indonesia film cinta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>asing</td>\n",
       "      <td>pasti telinga kita rasa asing dan aneh dengar ...</td>\n",
       "      <td>NN NN PRP VB JJ CC JJ VB NN VB NN NN Z SC SC J...</td>\n",
       "      <td>telinga asing aneh dengar menu masakan soto ke...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>dengar masakan jaja adalah rasa</td>\n",
       "      <td>menu soto kerbau telinga pasti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>asing</td>\n",
       "      <td>warga negara asing atau warga negara makmur ya...</td>\n",
       "      <td>NN NN JJ CC NN NN NN SC NN RB NEG CD MD VB NNP...</td>\n",
       "      <td>warga negara asing warga negara makmur kepala ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>jadi</td>\n",
       "      <td>warga negara makmur kepala negara warga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>asing</td>\n",
       "      <td>lama somenumber tahun perintah sultan mahmud j...</td>\n",
       "      <td>IN CD NN NN NN Z NNP NNP VB NN JJ IN NN JJ Z N...</td>\n",
       "      <td>perintah sultan mahmud jalin kerja asing belan...</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>jalin</td>\n",
       "      <td>dll pihak kerja perintah tahun belas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>121</td>\n",
       "      <td>asing</td>\n",
       "      <td>yang kemudian ikut dengan donatdonat waralaba ...</td>\n",
       "      <td>DT CC VB IN NN NN JJ RB IN NNP NNP Z NNP NNP Z...</td>\n",
       "      <td>donatdonat waralaba asing master ring master d...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>ikut</td>\n",
       "      <td>waralaba donat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   id   kata                                            kalimat  \\\n",
       "0           0   13  asing  para cinta film indonesia atau tv pasti tak as...   \n",
       "1           1   19  asing  pasti telinga kita rasa asing dan aneh dengar ...   \n",
       "2           2   41  asing  warga negara asing atau warga negara makmur ya...   \n",
       "3           3   44  asing  lama somenumber tahun perintah sultan mahmud j...   \n",
       "4           4  121  asing  yang kemudian ikut dengan donatdonat waralaba ...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0             DT NN NN NN CC NNP Z RB NEG JJ VB RB Z   \n",
       "1  NN NN PRP VB JJ CC JJ VB NN VB NN NN Z SC SC J...   \n",
       "2  NN NN JJ CC NN NN NN SC NN RB NEG CD MD VB NNP...   \n",
       "3  IN CD NN NN NN Z NNP NNP VB NN JJ IN NN JJ Z N...   \n",
       "4  DT CC VB IN NN NN JJ RB IN NNP NNP Z NNP NNP Z...   \n",
       "\n",
       "                                               clean  targetpos_clean  \\\n",
       "0          cinta film indonesia tv asing dengar nama                4   \n",
       "1  telinga asing aneh dengar menu masakan soto ke...                1   \n",
       "2  warga negara asing warga negara makmur kepala ...                2   \n",
       "3  perintah sultan mahmud jalin kerja asing belan...                5   \n",
       "4  donatdonat waralaba asing master ring master d...                2   \n",
       "\n",
       "   targetpos_ori  targetpos_pos_tag                            verbs  \\\n",
       "0              8                  9                           dengar   \n",
       "1              4                  4  dengar masakan jaja adalah rasa   \n",
       "2              2                  2                             jadi   \n",
       "3             11                 13                            jalin   \n",
       "4              6                  6                             ikut   \n",
       "\n",
       "                                     nouns  \n",
       "0                     indonesia film cinta  \n",
       "1           menu soto kerbau telinga pasti  \n",
       "2  warga negara makmur kepala negara warga  \n",
       "3     dll pihak kerja perintah tahun belas  \n",
       "4                           waralaba donat  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw = pd.read_csv('testing_data_clean.csv').fillna('')\n",
    "test_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>kata</th>\n",
       "      <th>kalimat</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>clean</th>\n",
       "      <th>targetpos_clean</th>\n",
       "      <th>targetpos_ori</th>\n",
       "      <th>targetpos_pos_tag</th>\n",
       "      <th>verbs</th>\n",
       "      <th>nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>631</td>\n",
       "      <td>76787</td>\n",
       "      <td>baru</td>\n",
       "      <td>namun teliti hadap peran dna di dalam sel baru...</td>\n",
       "      <td>CC Z NN IN NN NNP IN NN NN JJ VB IN NN NN CD Z...</td>\n",
       "      <td>teliti hadap peran dna dalam sel baru abad tem...</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>mulai sama</td>\n",
       "      <td>awal abad temu postulat sel dalam peran teliti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>632</td>\n",
       "      <td>77113</td>\n",
       "      <td>baru</td>\n",
       "      <td>tim atkinson milik main baru seperti jesper ol...</td>\n",
       "      <td>NNP NNP VB NN JJ IN NNP NNP Z NNP NNP CC NNP N...</td>\n",
       "      <td>tim atkinson milik main baru jesper olsen paul...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>main milik</td>\n",
       "      <td>main</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>633</td>\n",
       "      <td>77460</td>\n",
       "      <td>baru</td>\n",
       "      <td>dalam usia baru injak somenumber tahun malik t...</td>\n",
       "      <td>IN NN JJ VB CD NN Z NNP MD VB VB IN NN NNP Z</td>\n",
       "      <td>dalam usia baru injak malik niat pergi pulau jawa</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>injak niat pergi</td>\n",
       "      <td>tahun pulau usia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>634</td>\n",
       "      <td>77587</td>\n",
       "      <td>baru</td>\n",
       "      <td>dua tindak bersamasama mungkin lahir baru dala...</td>\n",
       "      <td>RB VB VB VB NN JJ IN NNP Z</td>\n",
       "      <td>tindak bersamasama lahir baru dalam kristus</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>mungkin sama tindak</td>\n",
       "      <td>lahir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>635</td>\n",
       "      <td>77813</td>\n",
       "      <td>baru</td>\n",
       "      <td>film pertama dari seri ini tayang perdana di e...</td>\n",
       "      <td>NN OD IN NN PR VB NN IN NNP NNP IN NNP Z NNP N...</td>\n",
       "      <td>film seri tayang perdana embassy theatre welli...</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>tayang</td>\n",
       "      <td>tanggal perdana seri film</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0     id  kata  \\\n",
       "631         631  76787  baru   \n",
       "632         632  77113  baru   \n",
       "633         633  77460  baru   \n",
       "634         634  77587  baru   \n",
       "635         635  77813  baru   \n",
       "\n",
       "                                               kalimat  \\\n",
       "631  namun teliti hadap peran dna di dalam sel baru...   \n",
       "632  tim atkinson milik main baru seperti jesper ol...   \n",
       "633  dalam usia baru injak somenumber tahun malik t...   \n",
       "634  dua tindak bersamasama mungkin lahir baru dala...   \n",
       "635  film pertama dari seri ini tayang perdana di e...   \n",
       "\n",
       "                                              pos_tags  \\\n",
       "631  CC Z NN IN NN NNP IN NN NN JJ VB IN NN NN CD Z...   \n",
       "632  NNP NNP VB NN JJ IN NNP NNP Z NNP NNP CC NNP N...   \n",
       "633       IN NN JJ VB CD NN Z NNP MD VB VB IN NN NNP Z   \n",
       "634                         RB VB VB VB NN JJ IN NNP Z   \n",
       "635  NN OD IN NN PR VB NN IN NNP NNP IN NNP Z NNP N...   \n",
       "\n",
       "                                                 clean  targetpos_clean  \\\n",
       "631  teliti hadap peran dna dalam sel baru abad tem...                6   \n",
       "632  tim atkinson milik main baru jesper olsen paul...                4   \n",
       "633  dalam usia baru injak malik niat pergi pulau jawa                2   \n",
       "634        tindak bersamasama lahir baru dalam kristus                3   \n",
       "635  film seri tayang perdana embassy theatre welli...                8   \n",
       "\n",
       "     targetpos_ori  targetpos_pos_tag                verbs  \\\n",
       "631              8                  9           mulai sama   \n",
       "632              4                  4           main milik   \n",
       "633              2                  2     injak niat pergi   \n",
       "634              5                  5  mungkin sama tindak   \n",
       "635             13                 14               tayang   \n",
       "\n",
       "                                              nouns  \n",
       "631  awal abad temu postulat sel dalam peran teliti  \n",
       "632                                            main  \n",
       "633                                tahun pulau usia  \n",
       "634                                           lahir  \n",
       "635                       tanggal perdana seri film  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = dict()\n",
    "for w in ambiguous_word:\n",
    "    test[w] = test_raw.query('kata == \"{}\"'.format(w))\n",
    "    \n",
    "test['baru'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_clf = {w: MajorLabelClassifier().fit(train[w], train[w].sense) for w in ambiguous_word}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_file = open('dummy_baseline_classification.csv', 'w')\n",
    "\n",
    "# for i in range(len(test_raw)):\n",
    "#     row = test_raw.iloc[i]\n",
    "#     res_file.write('{},{},{}'.format(row.id, row.word, dummy_clf[row.word].predict([None])[0]))\n",
    "    \n",
    "# res_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iacobacci, et. al (2016)\n",
    "Embeddings for Word Sense Disambiguation: An Evaluation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_TAGS_WINDOW = 2\n",
    "\n",
    "UNIGRAM = 0\n",
    "BIGRAM = 1\n",
    "\n",
    "collocation_pos = [\n",
    "    (-2, -2), (-1, -1), (1, 1), (2, 2), (-2, -1), (-1, 1), (1, 2),\n",
    "]\n",
    "\n",
    "collocation_type = [\n",
    "    UNIGRAM, UNIGRAM, UNIGRAM, UNIGRAM, BIGRAM, BIGRAM, BIGRAM\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGSET = [\n",
    "    '-', 'CC', 'CD', 'DT', 'FW', 'IN', 'JJ', 'MD', 'NEG', 'NN', 'UH',\n",
    "    'NND','NNP','OD','PR','PRP','RB','RP','SC','SYM','VB','WH','X','Z'\n",
    "]\n",
    "\n",
    "TAG_LABEL = {t: [1 if t == x else 0 for x in TAGSET] for t,i in zip(TAGSET, range(len(TAGSET)))}\n",
    "\n",
    "class POSTagTransformer(BaseEstimator, TransformerMixin):\n",
    "    def transform(self, X, y=None):\n",
    "        res = []\n",
    "        for sentence in X:\n",
    "            r = []\n",
    "            for tag in sentence:\n",
    "                r = [*r, *TAG_LABEL[tag]]\n",
    "            res.append(r)\n",
    "        res = np.array(res, dtype=np.bool)\n",
    "        return csr_matrix(res)\n",
    "\n",
    "\n",
    "def get_collocation(sentence, targetpos, L, R):\n",
    "    col = ['-' for i in range(R-L+1 - (1 if L < 0 and R > 0 else 0))]\n",
    "    tokens = sentence.split()\n",
    "    L = targetpos+L\n",
    "    R = targetpos+R\n",
    "    j = L\n",
    "    i = 0\n",
    "    while j <= R:\n",
    "        if j < 0:\n",
    "            j += 1\n",
    "            i += 1\n",
    "            continue\n",
    "        if j == targetpos:\n",
    "            j += 1\n",
    "            continue\n",
    "        if j >= len(tokens):\n",
    "            break\n",
    "        col[i] = tokens[j]\n",
    "        j += 1\n",
    "        i += 1\n",
    "    \n",
    "    return ' '.join(col)\n",
    "\n",
    "def get_collocation_vectors_vectorizer(dataset):\n",
    "    context_window = []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        instance = dataset.iloc[i]\n",
    "        context_window.append(get_collocation(instance.kalimat, instance.targetpos_ori, -2, 2))\n",
    "\n",
    "    unigram_vectorizer = CountVectorizer(ngram_range=(1,1), min_df=.0002).fit(context_window)\n",
    "    bigram_vectorizer = CountVectorizer(ngram_range=(2,2), min_df=.0002).fit(context_window)\n",
    "    \n",
    "    return unigram_vectorizer, bigram_vectorizer\n",
    "\n",
    "def get_collocation_vectors(vectorizers, dataset):\n",
    "    collocation_words = [[] for i in range(len(dataset))]\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        instance = dataset.iloc[i]\n",
    "        for l, r in collocation_pos:\n",
    "            collocation_words[i].append(get_collocation(instance.kalimat, instance.targetpos_ori, l, r))\n",
    "        \n",
    "    unigram_vectorizer, bigram_vectorizer = vectorizers\n",
    "    collocation_vectors = []\n",
    "\n",
    "    vectorizer = [None, None]\n",
    "    vectorizer[UNIGRAM] = unigram_vectorizer\n",
    "    vectorizer[BIGRAM] = bigram_vectorizer\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        vec = []\n",
    "        for j in range(len(collocation_pos)):\n",
    "            vec = [\n",
    "                *vec, \n",
    "                *np.array(vectorizer[collocation_type[j]].transform([collocation_words[i][j]]).toarray()[0], dtype=np.bool)\n",
    "            ]\n",
    "        collocation_vectors.append(vec)\n",
    "        \n",
    "    collocation_vectors = np.array(collocation_vectors, dtype=np.bool)\n",
    "\n",
    "    return csr_matrix(collocation_vectors)\n",
    "    \n",
    "\n",
    "\n",
    "def get_pos_tags(dataset):\n",
    "    pos_tags = [['-' for j in range(2*POS_TAGS_WINDOW+1)] for i in range(len(dataset))]\n",
    "    possible_tags = set()\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        row = dataset.iloc[i]\n",
    "        tags = row.pos_tags.split()\n",
    "        position = row.targetpos_pos_tag\n",
    "        pos_tags[i][POS_TAGS_WINDOW] = tags[position]\n",
    "        j = position-1\n",
    "        k = POS_TAGS_WINDOW - 1\n",
    "        while j >= 0 and j >= position - POS_TAGS_WINDOW:\n",
    "            if tags[j] == 'Z':\n",
    "                break # do not even include\n",
    "            pos_tags[i][k] = tags[j]\n",
    "            k -= 1\n",
    "            j -= 1\n",
    "        j = position+1\n",
    "        k = POS_TAGS_WINDOW + 1\n",
    "        while j < len(tags) and j <= position + POS_TAGS_WINDOW:\n",
    "            pos_tags[i][k] = tags[j]\n",
    "            if tags[j] == 'Z':\n",
    "                break # include, then break\n",
    "\n",
    "            k += 1\n",
    "            j += 1\n",
    "\n",
    "    return POSTagTransformer().transform(pos_tags)\n",
    "    \n",
    "def get_surrounding_words_vectorizer(dataset):\n",
    "    cv = CountVectorizer(min_df=.0002)\n",
    "    surrounding_words = cv.fit_transform(\n",
    "        list(map(lambda s: ' '.join(set(s.split())), dataset.clean))\n",
    "    )\n",
    "    return cv\n",
    "\n",
    "def get_surrounding_words(vectorizer, dataset):\n",
    "    return vectorizer.transform(\n",
    "        list(map(lambda s: ' '.join(set(s.split())), dataset.clean))\n",
    "    )\n",
    "\n",
    "def get_svd_transformer(X, size):\n",
    "    transformer = make_pipeline(TruncatedSVD(size), Normalizer(copy=False))\n",
    "    transformer.fit_transform(X)\n",
    "    return transformer\n",
    "    \n",
    "class ItMakesSenseFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        print('fitting pos tags.. ', end='')\n",
    "        pos_tags = get_pos_tags(X)\n",
    "        print('Done! | ', end='')\n",
    "#         print('\\n', pos_tags)\n",
    "        print('fitting collocation vectors.. ', end='')\n",
    "        self.collocation_unigram_vectorizer_, self.collocation_bigram_vectoirzer = (\n",
    "            get_collocation_vectors_vectorizer(X)\n",
    "        )\n",
    "        collocation_vectors = get_collocation_vectors(\n",
    "            (self.collocation_unigram_vectorizer_, self.collocation_bigram_vectoirzer),\n",
    "            X\n",
    "        )\n",
    "        print('Done! | ', end='')\n",
    "        print('fitting collocation vectors.. ', end='')\n",
    "        self.surrounding_words_vectorizer_ = (\n",
    "            get_surrounding_words_vectorizer(X)\n",
    "        )\n",
    "        surrounding_words = get_surrounding_words(\n",
    "            self.surrounding_words_vectorizer_,\n",
    "            X\n",
    "        )\n",
    "        print('Done! | ', end='')\n",
    "        print('fitting SVDs ', end='')\n",
    "        self.possvd_transformer_ = get_svd_transformer(pos_tags, 80)\n",
    "        self.imscvsvd_transformer_ = get_svd_transformer(collocation_vectors, 1000)\n",
    "        self.swsvd_transformer_ = get_svd_transformer(surrounding_words, 1000) \n",
    "        print('Done!')\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        pos_tags = get_pos_tags(X)\n",
    "        \n",
    "        collocation_vectors = get_collocation_vectors(\n",
    "            (self.collocation_unigram_vectorizer_, self.collocation_bigram_vectoirzer),\n",
    "            X\n",
    "        )\n",
    "        \n",
    "        surrounding_words = get_surrounding_words(\n",
    "            self.surrounding_words_vectorizer_,\n",
    "            X\n",
    "        )\n",
    "        \n",
    "        possvd = self.possvd_transformer_.transform(pos_tags)\n",
    "        imscvsvd = self.imscvsvd_transformer_.transform(collocation_vectors)\n",
    "        swsvd = self.swsvd_transformer_.transform(surrounding_words)\n",
    "        \n",
    "        return np.array(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda i: [*possvd[i], *imscvsvd[i], *swsvd[i]],\n",
    "                    [i for i in range(len(X))]\n",
    "                )\n",
    "            ), dtype=np.float32\n",
    "        )\n",
    "    \n",
    "\n",
    "class IacobacciFeatures(ItMakesSenseFeatures):\n",
    "    \n",
    "    def __init__(self, embedding_src=None, ims=True):\n",
    "        if embedding_src is None:\n",
    "            raise ValueError('Embeddings is required')\n",
    "        self.word_vectors_ = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(embedding_src)\n",
    "        self.embedding_size_ = self.word_vectors_.get_vector('wikipedia').shape[0]\n",
    "        self.embedding_src = embedding_src\n",
    "        self.ims = ims\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return super().fit(X, y) if self.ims else self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        ims = super().transform(X) if self.ims else [[] for i in range(len(X))]\n",
    "        \n",
    "        print('IMS feature extraction finished, now working on word embeddings.. ', end='')\n",
    "        \n",
    "        embedding = []\n",
    "\n",
    "        W = 5\n",
    "        alpha = 1 - (np.power(0.1, np.power(W-1.0, -1)))\n",
    "\n",
    "        for p in range(len(X)):\n",
    "            instance = X.iloc[p]\n",
    "            e = np.zeros(self.embedding_size_, dtype=np.float32)\n",
    "            I = instance.targetpos_ori\n",
    "            words = instance.kalimat.split()\n",
    "            for i in range(self.embedding_size_):\n",
    "                for j in range(max(0, I-W), min(len(words), I+W+1)):\n",
    "                    if j == I:\n",
    "                        continue\n",
    "                    try:\n",
    "                        e[i] += (self.word_vectors_.get_vector(words[j])[i] * (np.power(1 - alpha, abs(I-j) - 1)))\n",
    "                    except:\n",
    "                        continue\n",
    "            embedding.append(e)\n",
    "        embedding = np.array(embedding, dtype=np.float32)\n",
    "        \n",
    "        print('Done!')\n",
    "        \n",
    "        return np.array(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda i: [*ims[i], *embedding[i]],\n",
    "                    [i for i in range(len(X))]\n",
    "                )\n",
    "            ),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "class IacobacciExtendedFeatures(IacobacciFeatures):\n",
    "    def __init__(self, embedding_src=None, ims=True):\n",
    "        super().__init__(embedding_src, ims)\n",
    "        self.word_vectors_200_ = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(\n",
    "            '../wikipedia_indonesia_embedding200_more.model'\n",
    "        )\n",
    "        self.word_vectors_50_ = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(\n",
    "            '../wikipedia_indonesia_embedding50_more.model'\n",
    "        )\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        iaco = super().transform(X, y)\n",
    "        print('now working on extended features..', end='')\n",
    "        \n",
    "        verb_embeddings = []\n",
    "        noun_embeddings = []\n",
    "        embedding200 = []\n",
    "        embedding50 = []\n",
    "        \n",
    "        W = 5\n",
    "        alpha = 1 - (np.power(0.1, np.power(W-1.0, -1)))\n",
    "        \n",
    "        for p in range(len(X)):\n",
    "            instance = X.iloc[p]\n",
    "            verbs = instance.verbs.split()\n",
    "            nouns = instance.nouns.split()\n",
    "            try:\n",
    "                verb_embeddings.append(\n",
    "                    np.array(self.word_vectors_.get_vector(verbs[0]), dtype=np.float32) if len(verbs[0]) > 1 else np.zeros(self.embedding_size_)\n",
    "                )\n",
    "            except:\n",
    "                verb_embeddings.append(np.zeros(self.embedding_size_))\n",
    "            try:\n",
    "                noun_embeddings.append(\n",
    "                    np.array(self.word_vectors_.get_vector(nouns[0]), dtype=np.float32) if len(nouns[0]) > 1 else np.zeros(self.embedding_size_)\n",
    "                )\n",
    "            except:\n",
    "                noun_embeddings.append(np.zeros(self.embedding_size_))\n",
    "\n",
    "            e200 = np.zeros(200, dtype=np.float32)\n",
    "            e50 = np.zeros(50, dtype=np.float32)\n",
    "            \n",
    "            I = instance.targetpos_ori\n",
    "            words = instance.kalimat.split()\n",
    "            for i in range(200):\n",
    "                for j in range(max(0, I-W), min(len(words), I+W+1)):\n",
    "                    if j == I:\n",
    "                        continue\n",
    "                    try:\n",
    "                        e200[i] += (self.word_vectors_200_.get_vector(words[j])[i] * (np.power(1 - alpha, abs(I-j) - 1)))\n",
    "                    except:\n",
    "                        continue\n",
    "            for i in range(50):\n",
    "                for j in range(max(0, I-W), min(len(words), I+W+1)):\n",
    "                    if j == I:\n",
    "                        continue\n",
    "                    try:\n",
    "                        e50[i] += (self.word_vectors_50_.get_vector(words[j])[i] * (np.power(1 - alpha, abs(I-j) - 1)))\n",
    "                    except:\n",
    "                        continue\n",
    "            embedding200.append(e200)\n",
    "            embedding50.append(e50)\n",
    "            \n",
    "        embedding200 = np.array(embedding200, dtype=np.float32)\n",
    "        embedding50 = np.array(embedding50, dtype=np.float32)\n",
    "        verb_embeddings = np.array(verb_embeddings, dtype=np.float32)\n",
    "        noun_embeddings = np.array(noun_embeddings, dtype=np.float32)\n",
    "        print('Done!')\n",
    "        \n",
    "        return np.array(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda i: [*iaco[i], *embedding200[i], *embedding50[i], *verb_embeddings[i], *noun_embeddings[i]],\n",
    "                    [i for i in range(len(X))]\n",
    "                )\n",
    "            ),\n",
    "            dtype=np.float32\n",
    "        )            \n",
    "    \n",
    "        \n",
    "class WordExpertWSD(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    FEATURE_COLUMNS = {\n",
    "        'postag': np.arange(0, 80),\n",
    "        'collocations': np.arange(80, 1080),\n",
    "        'surrounding_words': np.arange(1080, 2080),\n",
    "        'embedding400': np.arange(2080, 2480),\n",
    "        'embedding200': np.arange(2480, 2680),\n",
    "        'embedding50': np.arange(2680, 2730),\n",
    "        'surrounding_verbs': np.arange(2730, 3130),\n",
    "        'surrounding_nouns': np.arange(3130, 3530)\n",
    "    }\n",
    "   \n",
    "    def __init__(\n",
    "        self,\n",
    "        use_collocations=True,\n",
    "        use_pos_tags=True,\n",
    "        use_surrounding_words=True,\n",
    "        embedding_size=400,\n",
    "        use_immediate_verbs=True,\n",
    "        use_immediate_nouns=True,\n",
    "        svm_c=1.0,\n",
    "        svm_max_iter=10\n",
    "    ):\n",
    "        self.svm_c = svm_c\n",
    "        self.svm_max_iter = svm_max_iter\n",
    "        self.use_collocations = use_collocations\n",
    "        self.use_pos_tags = use_pos_tags\n",
    "        self.use_surrounding_words = use_surrounding_words\n",
    "        self.embedding_size = embedding_size\n",
    "        self.use_immediate_verbs = use_immediate_verbs\n",
    "        self.use_immediate_nouns = use_immediate_nouns\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.estimator_ = LinearSVC(C=self.svm_c, max_iter=self.svm_max_iter)\n",
    "        return self.estimator_.fit(X.T[self._select_features()].T, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.estimator_.predict(X.T[self._select_features()].T)\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        return self.estimator_.decision_function(X.T[self._select_features()].T)\n",
    "    \n",
    "    def _select_features(self):\n",
    "        selectedFeatures = []\n",
    "        selectedFeatures = [*selectedFeatures, *(WordExpertWSD.FEATURE_COLUMNS['postag'] if self.use_pos_tags else [])]\n",
    "        selectedFeatures = [*selectedFeatures, *(WordExpertWSD.FEATURE_COLUMNS['collocations'] if self.use_collocations else [])]\n",
    "        selectedFeatures = [*selectedFeatures, *(WordExpertWSD.FEATURE_COLUMNS['surrounding_words'] if self.use_surrounding_words else [])]\n",
    "        selectedFeatures = [*selectedFeatures, *(WordExpertWSD.FEATURE_COLUMNS['embedding400'] if self.embedding_size == 400 else [])]\n",
    "        selectedFeatures = [*selectedFeatures, *(WordExpertWSD.FEATURE_COLUMNS['embedding200'] if self.embedding_size == 200  else [])]\n",
    "        selectedFeatures = [*selectedFeatures, *(WordExpertWSD.FEATURE_COLUMNS['embedding50'] if self.embedding_size == 50  else [])]\n",
    "        selectedFeatures = [*selectedFeatures, *(WordExpertWSD.FEATURE_COLUMNS['surrounding_verbs'] if self.use_immediate_verbs else [])]\n",
    "        selectedFeatures = [*selectedFeatures, *(WordExpertWSD.FEATURE_COLUMNS['surrounding_nouns'] if self.use_immediate_nouns else [])]\n",
    "        return selectedFeatures\n",
    "\n",
    "class WordSenseDisambiguator(LinearSVC):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Transformer\n",
    "Note: At this line of code, the rare label might already be dropped, double check this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_transformer = {w: LabelEncoder().fit(train_raw.query('kata == \"{}\"'.format(w)).sense) for w in ambiguous_word}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Classifier per Ambiguous Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = {w: None for w in ambiguous_word}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the Raw Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It Makes Sense (Zhong & Ng, 2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin = time.perf_counter()\n",
    "# imsTransformer = ItMakesSenseFeatures()\n",
    "# imsTransformer.fit(train_raw)\n",
    "# print('elapsed time:', time.perf_counter() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin = time.perf_counter()\n",
    "# X_train = imsTransformer.transform(train_raw)\n",
    "# print('elapsed time:', time.perf_counter() - begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iacobacci, et. al (2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting pos tags.. Done! | fitting collocation vectors.. Done! | fitting collocation vectors.. Done! | fitting SVDs Done!\n",
      "elapsed time: 71.63781664999988\n"
     ]
    }
   ],
   "source": [
    "begin = time.perf_counter()\n",
    "iaco = IacobacciFeatures(embedding_src='../wikipedia_indonesia_embedding400_more.model', ims=True)\n",
    "iaco.fit(train_raw)\n",
    "print('elapsed time:', time.perf_counter() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "elapsed time: 197.32473079199917\n"
     ]
    }
   ],
   "source": [
    "begin = time.perf_counter()\n",
    "X_train = iaco.transform(train_raw)\n",
    "print('elapsed time:', time.perf_counter() - begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Iacobacci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nakama/Documents/StateOfTheArtWSD/nlp/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting pos tags.. Done! | fitting collocation vectors.. Done! | fitting collocation vectors.. Done! | fitting SVDs Done!\n",
      "elapsed time: 66.22435734800001\n"
     ]
    }
   ],
   "source": [
    "begin = time.perf_counter()\n",
    "iaco = IacobacciExtendedFeatures(embedding_src='../wikipedia_indonesia_embedding400_more.model', ims=True)\n",
    "iaco.fit(train_raw)\n",
    "print('elapsed time:', time.perf_counter() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "now working on extended features..Done!\n",
      "elapsed time: 305.97306678099994\n"
     ]
    }
   ],
   "source": [
    "begin = time.perf_counter()\n",
    "X_train = iaco.transform(train_raw)\n",
    "print('elapsed time:', time.perf_counter() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8379, 2480)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1450,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][2080:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([label_transformer[w].transform([y])[0] for w, y in zip(list(train_raw.kata), list(train_raw.sense))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM\n",
      "==================================\n",
      "asing\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.7623089241113075\n",
      "Dummy classifier f1-score:  0.4825174825174825\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 20\n",
      "elapsed time: 3.134761356999661\n",
      "----------------------------------\n",
      "==================================\n",
      "atas\n",
      "\n",
      "Training f1-score: 0.968204134366925\n",
      "Cross validation f1-score: 0.40982164077752314\n",
      "Dummy classifier f1-score:  0.050970873786407765\n",
      "Best parameters:\n",
      "C : 0.5\n",
      "max_iter : 10\n",
      "elapsed time: 3.8503641550014436\n",
      "----------------------------------\n",
      "==================================\n",
      "badan\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.7653465829615256\n",
      "Dummy classifier f1-score:  0.27255985267034993\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 20\n",
      "elapsed time: 0.8700690500008932\n",
      "----------------------------------\n",
      "==================================\n",
      "baru\n",
      "\n",
      "Training f1-score: 0.9936531335094076\n",
      "Cross validation f1-score: 0.5516477281583665\n",
      "Dummy classifier f1-score:  0.284037558685446\n",
      "Best parameters:\n",
      "C : 8.0\n",
      "max_iter : 10\n",
      "elapsed time: 1.8814094570007\n",
      "----------------------------------\n",
      "==================================\n",
      "berat\n",
      "\n",
      "Training f1-score: 0.9913871635610766\n",
      "Cross validation f1-score: 0.46279953001934426\n",
      "Dummy classifier f1-score:  0.10769230769230768\n",
      "Best parameters:\n",
      "C : 0.5\n",
      "max_iter : 20\n",
      "elapsed time: 1.7847442000020237\n",
      "----------------------------------\n",
      "==================================\n",
      "besar\n",
      "\n",
      "Training f1-score: 0.9964745022096431\n",
      "Cross validation f1-score: 0.479017824609855\n",
      "Dummy classifier f1-score:  0.15732758620689655\n",
      "Best parameters:\n",
      "C : 8.0\n",
      "max_iter : 20\n",
      "elapsed time: 2.2246978689981916\n",
      "----------------------------------\n",
      "==================================\n",
      "bidang\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.6848234471985859\n",
      "Dummy classifier f1-score:  0.4854014598540146\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 10\n",
      "elapsed time: 1.4926036780016148\n",
      "----------------------------------\n",
      "==================================\n",
      "bintang\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.6605256605566203\n",
      "Dummy classifier f1-score:  0.24113475177304963\n",
      "Best parameters:\n",
      "C : 8.0\n",
      "max_iter : 20\n",
      "elapsed time: 1.3083740639995085\n",
      "----------------------------------\n",
      "==================================\n",
      "bisa\n",
      "\n",
      "Training f1-score: 0.9619177150629037\n",
      "Cross validation f1-score: 0.4970978283786633\n",
      "Dummy classifier f1-score:  0.43718592964824116\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 10\n",
      "elapsed time: 1.444844932997512\n",
      "----------------------------------\n",
      "==================================\n",
      "buah\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.8159513872847206\n",
      "Dummy classifier f1-score:  0.2397003745318352\n",
      "Best parameters:\n",
      "C : 8.0\n",
      "max_iter : 20\n",
      "elapsed time: 1.4557801379996818\n",
      "----------------------------------\n",
      "==================================\n",
      "bulan\n",
      "\n",
      "Training f1-score: 0.9533951390628914\n",
      "Cross validation f1-score: 0.8573965629494067\n",
      "Dummy classifier f1-score:  0.45614035087719296\n",
      "Best parameters:\n",
      "C : 1.0\n",
      "max_iter : 10\n",
      "elapsed time: 2.0457344200003718\n",
      "----------------------------------\n",
      "==================================\n",
      "bunga\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.6686502863253564\n",
      "Dummy classifier f1-score:  0.3101343101343101\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 10\n",
      "elapsed time: 1.2005303680016368\n",
      "----------------------------------\n",
      "==================================\n",
      "cabang\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.691562681398747\n",
      "Dummy classifier f1-score:  0.3177570093457944\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 10\n",
      "elapsed time: 1.2637074109989044\n",
      "----------------------------------\n",
      "==================================\n",
      "cerah\n",
      "\n",
      "Training f1-score: 0.9617595935285467\n",
      "Cross validation f1-score: 0.5439370311300136\n",
      "Dummy classifier f1-score:  0.31186440677966104\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 10\n",
      "elapsed time: 1.1019441630014626\n",
      "----------------------------------\n",
      "==================================\n",
      "coklat\n",
      "\n",
      "Training f1-score: 0.9914311524067623\n",
      "Cross validation f1-score: 0.7070670440454194\n",
      "Dummy classifier f1-score:  0.3037974683544304\n",
      "Best parameters:\n",
      "C : 0.5\n",
      "max_iter : 20\n",
      "elapsed time: 1.2043688710000424\n",
      "----------------------------------\n",
      "==================================\n",
      "dalam\n",
      "\n",
      "Training f1-score: 0.9480593831896866\n",
      "Cross validation f1-score: 0.24612188483354608\n",
      "Dummy classifier f1-score:  0.060714285714285714\n",
      "Best parameters:\n",
      "C : 0.5\n",
      "max_iter : 10\n",
      "elapsed time: 3.6512997659992834\n",
      "----------------------------------\n",
      "==================================\n",
      "dasar\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.49258438320938325\n",
      "Dummy classifier f1-score:  0.176056338028169\n",
      "Best parameters:\n",
      "C : 1.0\n",
      "max_iter : 40\n",
      "elapsed time: 2.1222719779980252\n",
      "----------------------------------\n",
      "==================================\n",
      "dunia\n",
      "\n",
      "Training f1-score: 0.9782787346617134\n",
      "Cross validation f1-score: 0.36655522309934074\n",
      "Dummy classifier f1-score:  0.1794871794871795\n",
      "Best parameters:\n",
      "C : 4.0\n",
      "max_iter : 10\n",
      "elapsed time: 1.8539199260012538\n",
      "----------------------------------\n",
      "==================================\n",
      "halaman\n",
      "\n",
      "Training f1-score: 0.9256704980842912\n",
      "Cross validation f1-score: 0.6771694048688955\n",
      "Dummy classifier f1-score:  0.24637681159420288\n",
      "Best parameters:\n",
      "C : 1.0\n",
      "max_iter : 10\n",
      "elapsed time: 1.442316717999347\n",
      "----------------------------------\n",
      "==================================\n",
      "harapan\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.5572967772967773\n",
      "Dummy classifier f1-score:  0.2818428184281843\n",
      "Best parameters:\n",
      "C : 0.5\n",
      "max_iter : 40\n",
      "elapsed time: 1.8726622340000176\n",
      "----------------------------------\n",
      "==================================\n",
      "jalan\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.46479003729003726\n",
      "Dummy classifier f1-score:  0.1651376146788991\n",
      "Best parameters:\n",
      "C : 4.0\n",
      "max_iter : 40\n",
      "elapsed time: 1.8707516069989651\n",
      "----------------------------------\n",
      "==================================\n",
      "jam\n",
      "\n",
      "Training f1-score: 0.9793072669482301\n",
      "Cross validation f1-score: 0.7136611904712032\n",
      "Dummy classifier f1-score:  0.17924528301886794\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 40\n",
      "elapsed time: 2.6056560580000223\n",
      "----------------------------------\n",
      "==================================\n",
      "jaringan\n",
      "\n",
      "Training f1-score: 0.9502527056706623\n",
      "Cross validation f1-score: 0.7702389959281123\n",
      "Dummy classifier f1-score:  0.20202020202020202\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 10\n",
      "elapsed time: 2.1518139420004445\n",
      "----------------------------------\n",
      "==================================\n",
      "kabur\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.4652137891286337\n",
      "Dummy classifier f1-score:  0.3182674199623352\n",
      "Best parameters:\n",
      "C : 0.5\n",
      "max_iter : 20\n",
      "elapsed time: 1.677943865000998\n",
      "----------------------------------\n",
      "==================================\n",
      "kaki\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.694029569464352\n",
      "Dummy classifier f1-score:  0.17857142857142858\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 40\n",
      "elapsed time: 1.0605137809980079\n",
      "----------------------------------\n",
      "==================================\n",
      "kali\n",
      "\n",
      "Training f1-score: 0.9614323038788772\n",
      "Cross validation f1-score: 0.6174984662001154\n",
      "Dummy classifier f1-score:  0.13538461538461538\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 10\n",
      "elapsed time: 2.6621583539999847\n",
      "----------------------------------\n",
      "==================================\n",
      "kepala\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.6136472332698748\n",
      "Dummy classifier f1-score:  0.3046964490263459\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 10\n",
      "elapsed time: 1.1977071559995238\n",
      "----------------------------------\n",
      "==================================\n",
      "ketat\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.7406594017725961\n",
      "Dummy classifier f1-score:  0.1534090909090909\n",
      "Best parameters:\n",
      "C : 1.0\n",
      "max_iter : 20\n",
      "elapsed time: 2.520074131000001\n",
      "----------------------------------\n",
      "==================================\n",
      "kulit\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.6660494158055134\n",
      "Dummy classifier f1-score:  0.26506024096385544\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 40\n",
      "elapsed time: 1.8281192750000628\n",
      "----------------------------------\n",
      "==================================\n",
      "kunci\n",
      "\n",
      "Training f1-score: 0.9885340175174733\n",
      "Cross validation f1-score: 0.39709231996522343\n",
      "Dummy classifier f1-score:  0.11588785046728971\n",
      "Best parameters:\n",
      "C : 0.5\n",
      "max_iter : 20\n",
      "elapsed time: 2.5802361210007803\n",
      "----------------------------------\n",
      "==================================\n",
      "layar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training f1-score: 0.9869275461380724\n",
      "Cross validation f1-score: 0.8753261680847887\n",
      "Dummy classifier f1-score:  0.3347826086956522\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 10\n",
      "elapsed time: 2.0833364819991402\n",
      "----------------------------------\n",
      "==================================\n",
      "lebat\n",
      "\n",
      "Training f1-score: 0.9940143263664017\n",
      "Cross validation f1-score: 0.9260558541987113\n",
      "Dummy classifier f1-score:  0.3920265780730897\n",
      "Best parameters:\n",
      "C : 0.5\n",
      "max_iter : 20\n",
      "elapsed time: 2.2163405170031183\n",
      "----------------------------------\n",
      "==================================\n",
      "lingkungan\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.605326407796996\n",
      "Dummy classifier f1-score:  0.2222222222222222\n",
      "Best parameters:\n",
      "C : 1.0\n",
      "max_iter : 40\n",
      "elapsed time: 1.7503375449996383\n",
      "----------------------------------\n",
      "==================================\n",
      "mata\n",
      "\n",
      "Training f1-score: 0.9947201336675022\n",
      "Cross validation f1-score: 0.6449668081727578\n",
      "Dummy classifier f1-score:  0.11228070175438598\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 20\n",
      "elapsed time: 1.7648956609991728\n",
      "----------------------------------\n",
      "==================================\n",
      "membawa\n",
      "\n",
      "Training f1-score: 0.9569962686567165\n",
      "Cross validation f1-score: 0.33992981906294595\n",
      "Dummy classifier f1-score:  0.05442176870748299\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 10\n",
      "elapsed time: 3.9883742870006245\n",
      "----------------------------------\n",
      "==================================\n",
      "memecahkan\n",
      "\n",
      "Training f1-score: 0.9921773142112125\n",
      "Cross validation f1-score: 0.7658478384340454\n",
      "Dummy classifier f1-score:  0.15999999999999998\n",
      "Best parameters:\n",
      "C : 4.0\n",
      "max_iter : 10\n",
      "elapsed time: 1.993345525002951\n",
      "----------------------------------\n",
      "==================================\n",
      "menangkap\n",
      "\n",
      "Training f1-score: 0.976273508531573\n",
      "Cross validation f1-score: 0.5181926406926407\n",
      "Dummy classifier f1-score:  0.21686746987951808\n",
      "Best parameters:\n",
      "C : 2.0\n",
      "max_iter : 10\n",
      "elapsed time: 1.532948721996945\n",
      "----------------------------------\n",
      "==================================\n",
      "mendorong\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.7692960141994384\n",
      "Dummy classifier f1-score:  0.4672364672364672\n",
      "Best parameters:\n",
      "C : 0.5\n",
      "max_iter : 10\n",
      "elapsed time: 2.0686557769986393\n",
      "----------------------------------\n",
      "==================================\n",
      "menerima\n",
      "\n",
      "Training f1-score: 0.8743226834869059\n",
      "Cross validation f1-score: 0.2629867631667464\n",
      "Dummy classifier f1-score:  0.11327433628318584\n",
      "Best parameters:\n",
      "C : 4.0\n",
      "max_iter : 10\n",
      "elapsed time: 2.8554467239991936\n",
      "----------------------------------\n",
      "==================================\n",
      "mengandung\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.7934011934011933\n",
      "Dummy classifier f1-score:  0.4895833333333333\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 20\n",
      "elapsed time: 2.0086450859998877\n",
      "----------------------------------\n",
      "==================================\n",
      "mengejar\n",
      "\n",
      "Training f1-score: 0.9607039537126327\n",
      "Cross validation f1-score: 0.8330686624572031\n",
      "Dummy classifier f1-score:  0.38257575757575757\n",
      "Best parameters:\n",
      "C : 2.0\n",
      "max_iter : 20\n",
      "elapsed time: 2.3241707950001\n",
      "----------------------------------\n",
      "==================================\n",
      "mengeluarkan\n",
      "\n",
      "Training f1-score: 0.967458195399372\n",
      "Cross validation f1-score: 0.4933338257484599\n",
      "Dummy classifier f1-score:  0.11473429951690821\n",
      "Best parameters:\n",
      "C : 0.5\n",
      "max_iter : 10\n",
      "elapsed time: 3.3076222780000535\n",
      "----------------------------------\n",
      "==================================\n",
      "mengikat\n",
      "\n",
      "Training f1-score: 0.9135772512354393\n",
      "Cross validation f1-score: 0.47983284911228063\n",
      "Dummy classifier f1-score:  0.10847457627118644\n",
      "Best parameters:\n",
      "C : 2.0\n",
      "max_iter : 10\n",
      "elapsed time: 3.145047993002663\n",
      "----------------------------------\n",
      "==================================\n",
      "mengisi\n",
      "\n",
      "Training f1-score: 0.909478128819586\n",
      "Cross validation f1-score: 0.5055783174508386\n",
      "Dummy classifier f1-score:  0.14791666666666667\n",
      "Best parameters:\n",
      "C : 2.0\n",
      "max_iter : 10\n",
      "elapsed time: 2.7628493560005154\n",
      "----------------------------------\n",
      "==================================\n",
      "menjaga\n",
      "\n",
      "Training f1-score: 0.8483249952734835\n",
      "Cross validation f1-score: 0.3563643426516426\n",
      "Dummy classifier f1-score:  0.1634980988593156\n",
      "Best parameters:\n",
      "C : 1.0\n",
      "max_iter : 10\n",
      "elapsed time: 3.13923900699956\n",
      "----------------------------------\n",
      "==================================\n",
      "menurunkan\n",
      "\n",
      "Training f1-score: 0.9938026378515811\n",
      "Cross validation f1-score: 0.38490125703978084\n",
      "Dummy classifier f1-score:  0.12656641604010024\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 20\n",
      "elapsed time: 2.867748477998248\n",
      "----------------------------------\n",
      "==================================\n",
      "menyusun\n",
      "\n",
      "Training f1-score: 0.99532195512095\n",
      "Cross validation f1-score: 0.5793152232969166\n",
      "Dummy classifier f1-score:  0.24242424242424243\n",
      "Best parameters:\n",
      "C : 4.0\n",
      "max_iter : 20\n",
      "elapsed time: 2.15572092200091\n",
      "----------------------------------\n",
      "==================================\n",
      "nilai\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.665051921762448\n",
      "Dummy classifier f1-score:  0.12085308056872036\n",
      "Best parameters:\n",
      "C : 8.0\n",
      "max_iter : 20\n",
      "elapsed time: 2.432541082998796\n",
      "----------------------------------\n",
      "==================================\n",
      "panas\n",
      "\n",
      "Training f1-score: 0.967251461988304\n",
      "Cross validation f1-score: 0.7260754675506248\n",
      "Dummy classifier f1-score:  0.23008849557522124\n",
      "Best parameters:\n",
      "C : 2.0\n",
      "max_iter : 10\n",
      "elapsed time: 1.4088911910002935\n",
      "----------------------------------\n",
      "==================================\n",
      "pembagian\n",
      "\n",
      "Training f1-score: 0.9896049896049895\n",
      "Cross validation f1-score: 0.3761550977757875\n",
      "Dummy classifier f1-score:  0.13581395348837208\n",
      "Best parameters:\n",
      "C : 0.5\n",
      "max_iter : 40\n",
      "elapsed time: 2.472052753000753\n",
      "----------------------------------\n",
      "==================================\n",
      "rapat\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.7271145718944461\n",
      "Dummy classifier f1-score:  0.3021914648212226\n",
      "Best parameters:\n",
      "C : 0.5\n",
      "max_iter : 10\n",
      "elapsed time: 1.2040165079997678\n",
      "----------------------------------\n",
      "==================================\n",
      "sarung\n",
      "\n",
      "Training f1-score: 0.9891254315304947\n",
      "Cross validation f1-score: 0.9174770085902813\n",
      "Dummy classifier f1-score:  0.37\n",
      "Best parameters:\n",
      "C : 0.25\n",
      "max_iter : 10\n",
      "elapsed time: 2.2703254549996927\n",
      "----------------------------------\n",
      "==================================\n",
      "tengah\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.6723654123654124\n",
      "Dummy classifier f1-score:  0.1624203821656051\n",
      "Best parameters:\n",
      "C : 0.5\n",
      "max_iter : 10\n",
      "elapsed time: 1.4546684240012837\n",
      "----------------------------------\n",
      "==================================\n",
      "tinggi\n",
      "\n",
      "Training f1-score: 1.0\n",
      "Cross validation f1-score: 0.4078161777531526\n",
      "Dummy classifier f1-score:  0.06945540647198105\n",
      "Best parameters:\n",
      "C : 1.0\n",
      "max_iter : 40\n",
      "elapsed time: 2.6186523280011897\n",
      "----------------------------------\n",
      "Cross validation macro average f1-score: 0.6049322573179372\n",
      "Dummy classifier macro average f1-score: 0.234446068106352\n",
      "elapsed time: 113.2024081999989\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Select best parameter using k-fold cross validation\n",
    "'''\n",
    "\n",
    "def train_f1(X, y, clf, possible_param, fold=5):\n",
    "    clf = GridSearchCV(clf, possible_param, cv=fold, n_jobs=-1, iid=False, scoring='f1_macro')\n",
    "    clf.fit(X, y)\n",
    "    label_counts = np.bincount(y)\n",
    "    most_freq_label = np.argmax(label_counts)\n",
    "    print()\n",
    "#     train_score = clf.score(X, y)\n",
    "    train_score = classification_report(y, clf.predict(X), output_dict=True)['macro avg']['f1-score']\n",
    "    print('Training f1-score:', train_score)\n",
    "    print('Cross validation f1-score:', clf.best_score_)\n",
    "#     dummy_score = label_counts[most_freq_label] / len(y)\n",
    "    dummy_score = classification_report(y, [most_freq_label for i in y], output_dict=True)['macro avg']['f1-score']\n",
    "    print('Dummy classifier f1-score: ', dummy_score)\n",
    "    print_param(clf.best_params_)\n",
    "    return (clf.best_estimator_, clf.best_score_, train_score, dummy_score)\n",
    "\n",
    "def print_param(param):\n",
    "    print('Best parameters:')\n",
    "    for p in param:\n",
    "        print(p, ':', param[p])\n",
    "\n",
    "def train_all_f1(clf, possible_param, fold=5, algorithm_name=''):\n",
    "    print(algorithm_name)\n",
    "    words = []\n",
    "    train_scores = []\n",
    "    scores = []\n",
    "    dummy_scores = []\n",
    "    for w in sorted(classifier.keys()):\n",
    "        print('==================================')\n",
    "        print(w)\n",
    "        begin = time.perf_counter()\n",
    "        indexes = list(train_raw.query('kata == \"{}\"'.format(w)).index)\n",
    "        if len(set(y_train[indexes])) > 1:\n",
    "            best_clf, best_score, train_score, dummy_score = train_f1(X_train[indexes], y_train[indexes], clf, possible_param, fold)\n",
    "        else:\n",
    "            print('only one label detected')\n",
    "            best_clf = MajorLabelClassifier()\n",
    "            best_clf.fit(X_train[indexes], y_train[indexes])\n",
    "            best_score = classification_report(y_train[indexes], best_clf.predict(X_train[indexes]), output_dict=True)['macro avg']['f1-score']\n",
    "            train_score = best_score\n",
    "            dummy_score = train_score\n",
    "            print('Dummy classifier f1-score: ', dummy_score)\n",
    "        scores.append(best_score)\n",
    "        train_scores.append(train_score)\n",
    "        dummy_scores.append(dummy_score)\n",
    "        words.append(w)\n",
    "        classifier[w] = best_clf\n",
    "        print('elapsed time:', time.perf_counter() - begin)\n",
    "        print('----------------------------------')\n",
    "    print('Cross validation macro average f1-score:', sum(scores)/len(scores))\n",
    "    print('Dummy classifier macro average f1-score:', sum(dummy_scores)/len(dummy_scores))\n",
    "    return pd.DataFrame({\n",
    "        'word': words,\n",
    "        'train_acc': train_scores,\n",
    "        '{}-fold_validation_macro_f1'.format(fold): scores,\n",
    "        'dummy_acc': dummy_scores\n",
    "    })\n",
    "    \n",
    "begin = time.perf_counter()\n",
    "train_result = train_all_f1(\n",
    "    WordSenseDisambiguator(),\n",
    "    {'max_iter': [10, 20, 40], 'C':[0.25, 0.5, 1.0, 2.0, 4.0, 8.0]},\n",
    "    algorithm_name='Linear SVM'\n",
    ")\n",
    "print('elapsed time:', time.perf_counter() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result.to_csv('iacobacci_better_impl3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grow the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select which ambiguous word to grow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5201 : 35\n",
      "5202 : 20\n",
      "5204 : 11\n",
      "5203 : 12\n",
      "5206 : 17\n"
     ]
    }
   ],
   "source": [
    "chosen_word = 'berat'\n",
    "portion = train_raw.query('kata == \"{}\"'.format(chosen_word))\n",
    "for s in set(portion.sense):\n",
    "    print(s, ':', list(portion.sense).count(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select sense to grow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "grow_sense = ['5204', '5203', '5206', '5202', '5201']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMS feature extraction finished, now working on word embeddings.. Done!\n"
     ]
    }
   ],
   "source": [
    "untagged_dataset = pd.read_csv('../semi supervised dataset/{}_untagged.csv'.format(chosen_word))\n",
    "X_untagged = iaco.transform(untagged_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow(w, senses, target_growth):\n",
    "    untagged_decision_fx = classifier[w].decision_function(X_untagged)\n",
    "    pred_label = classifier[w].predict(X_untagged)\n",
    "    pred_confidence = [\n",
    "        abs(untagged_decision_fx[j]) if untagged_decision_fx.ndim == 1 \n",
    "        else abs(untagged_decision_fx[j][pred_label[j]]) \n",
    "        for j in range(len(X_untagged))\n",
    "    ]\n",
    "    pred_label = label_transformer[w].inverse_transform(pred_label)\n",
    "    untagged_dataset['sense'] = pred_label\n",
    "    pred = sorted([\n",
    "            (label, confidence, idx) for label, confidence, idx in zip(pred_label, pred_confidence, range(len(X_untagged)))\n",
    "        ], key=lambda p: p[1], reverse=True\n",
    "    )\n",
    "    growth_count = {s: 0 for s in senses}\n",
    "    selected = []\n",
    "    for label, confidence, idx in pred:\n",
    "        if label in growth_count.keys() and growth_count[label] < target_growth:\n",
    "            growth_count[label] += 1\n",
    "            selected.append(idx)\n",
    "    return untagged_dataset.iloc[selected], X_untagged[selected], label_transformer[w].transform(untagged_dataset.iloc[selected].sense)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 0.03571163500055263\n"
     ]
    }
   ],
   "source": [
    "new_dataset, new_X, new_y = grow(chosen_word, grow_sense, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_dataset.to_csv('../semi supervised dataset/{}_growed.csv'.format(chosen_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kata</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>kalimat</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>clean</th>\n",
       "      <th>targetpos_ori</th>\n",
       "      <th>targetpos_clean</th>\n",
       "      <th>targetpos_pos_tag</th>\n",
       "      <th>sense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>berat</td>\n",
       "      <td>24301</td>\n",
       "      <td>posthardcore kembang di amerika serikat khusus...</td>\n",
       "      <td>NNP VB IN NNP NNP Z RB IN NNP CC NNP NNP NNP Z...</td>\n",
       "      <td>posthardcore kembang amerika serikat khusus ch...</td>\n",
       "      <td>26</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>5206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>berat</td>\n",
       "      <td>200743</td>\n",
       "      <td>walaupun sangat tarik dengan dunia televisi ge...</td>\n",
       "      <td>SC RB VB IN NN NN Z VB NN NN NNP PR VB RB IN N...</td>\n",
       "      <td>tarik dunia televisi gemar berat musik jazz ka...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>5206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>berat</td>\n",
       "      <td>902080</td>\n",
       "      <td>horan juga ungkap kalau dia rupa gemar berat m...</td>\n",
       "      <td>NN RB VB SC PRP VB NN NN NN JJ Z VB NNP NNP Z ...</td>\n",
       "      <td>horan rupa gemar berat musik swing frank sinat...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>5206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>berat</td>\n",
       "      <td>266058</td>\n",
       "      <td>film ini kisah tentang orang anggur kelas bera...</td>\n",
       "      <td>NN PR VB IN NND NN NN JJ VB NNP NNP Z NNP NNP Z Z</td>\n",
       "      <td>film kisah orang anggur kelas berat nama tino ...</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>berat</td>\n",
       "      <td>980501</td>\n",
       "      <td>pada tanggal somenumber juni somenumber james ...</td>\n",
       "      <td>IN NN CD NNP CD Z NNP VB NNP SC RB NEG VB SC V...</td>\n",
       "      <td>tanggal juni james kalah max kalah juara kelas...</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>5206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      kata  discourse_id                                            kalimat  \\\n",
       "140  berat         24301  posthardcore kembang di amerika serikat khusus...   \n",
       "450  berat        200743  walaupun sangat tarik dengan dunia televisi ge...   \n",
       "845  berat        902080  horan juga ungkap kalau dia rupa gemar berat m...   \n",
       "524  berat        266058  film ini kisah tentang orang anggur kelas bera...   \n",
       "856  berat        980501  pada tanggal somenumber juni somenumber james ...   \n",
       "\n",
       "                                              pos_tags  \\\n",
       "140  NNP VB IN NNP NNP Z RB IN NNP CC NNP NNP NNP Z...   \n",
       "450  SC RB VB IN NN NN Z VB NN NN NNP PR VB RB IN N...   \n",
       "845  NN RB VB SC PRP VB NN NN NN JJ Z VB NNP NNP Z ...   \n",
       "524  NN PR VB IN NND NN NN JJ VB NNP NNP Z NNP NNP Z Z   \n",
       "856  IN NN CD NNP CD Z NNP VB NNP SC RB NEG VB SC V...   \n",
       "\n",
       "                                                 clean  targetpos_ori  \\\n",
       "140  posthardcore kembang amerika serikat khusus ch...             26   \n",
       "450  tarik dunia televisi gemar berat musik jazz ka...              7   \n",
       "845  horan rupa gemar berat musik swing frank sinat...              7   \n",
       "524  film kisah orang anggur kelas berat nama tino ...              7   \n",
       "856  tanggal juni james kalah max kalah juara kelas...             16   \n",
       "\n",
       "     targetpos_clean  targetpos_pos_tag sense  \n",
       "140               15                 29  5206  \n",
       "450                4                  8  5206  \n",
       "845                3                  7  5206  \n",
       "524                5                  7  5206  \n",
       "856                8                 17  5206  "
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_indexes = list(train_raw.query('kata == \"{}\"'.format(chosen_word)).index)\n",
    "X = [*X_train[target_indexes], *[]]\n",
    "y = [*y_train[target_indexes], *[]]\n",
    "classifier[chosen_word] = classifier[chosen_word].fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Growing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kepala\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "buah\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "mengeluarkan\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "mengejar\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "memecahkan\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "panas\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "rapat\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "nilai\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "pembagian\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "jaringan\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "asing\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "menjaga\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "berat\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "badan\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "lebat\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "kaki\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "tengah\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "harapan\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "bulan\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "cerah\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "layar\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "dasar\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "menyusun\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "tinggi\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "kabur\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "jam\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "kunci\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "baru\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "coklat\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "menangkap\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "besar\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "jalan\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "bunga\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "menerima\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "ketat\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "menurunkan\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "membawa\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "mata\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "halaman\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "bidang\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "mendorong\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "mengikat\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "kali\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "dunia\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "bisa\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "kulit\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "mengandung\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "sarung\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "lingkungan\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "bintang\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "mengisi\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "cabang\n",
      "IMS feature extraction finished, now working on word embeddings.. Done!\n"
     ]
    }
   ],
   "source": [
    "exception_words = '''\n",
    "dalam atas\n",
    "'''.split()\n",
    "\n",
    "cnt = 100\n",
    "\n",
    "for w in ambiguous_word:\n",
    "    if w in exception_words:\n",
    "        continue\n",
    "    print(w)\n",
    "    portion = train_raw.query('kata == \"{}\"'.format(w))\n",
    "    sense_count = {s: 0 for s in set(portion.sense)}\n",
    "    for s in set(portion.sense):\n",
    "        sense_count[s] = list(portion.sense).count(s)\n",
    "    untagged_dataset = pd.read_csv('../semi supervised dataset/{}_untagged.csv'.format(w))\n",
    "    X_untagged = iaco.transform(untagged_dataset)\n",
    "    target_indexes = list(train_raw.query('kata == \"{}\"'.format(w)).index)\n",
    "    new_dataset = None\n",
    "    for t in range(50):\n",
    "        growing_sense = [s for s in sense_count.keys() if sense_count[s] < t]\n",
    "        new_dataset, new_X, new_y = grow(w, growing_sense, t)\n",
    "        X = [*X_train[target_indexes], *new_X]\n",
    "        y = [*y_train[target_indexes], *new_y]\n",
    "        classifier[chosen_word] = classifier[w].fit(X, y)\n",
    "    new_dataset.to_csv('../semi supervised dataset/{}_growed.csv'.format(w))\n",
    "    filehandler = open('semi_supervisedv{}.model'.format(cnt), 'wb') \n",
    "    pickle.dump(classifier, filehandler)\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMS feature extraction finished, now working on word embeddings.. Done!\n",
      "elapsed time: 198.3335736709996\n"
     ]
    }
   ],
   "source": [
    "begin = time.perf_counter()\n",
    "X_test = iaco.transform(test_raw)\n",
    "print('elapsed time:', time.perf_counter() - begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 99.99 % | Time elapsed: 28.56120296200242655"
     ]
    }
   ],
   "source": [
    "res_file = open('../semisupervised_wsd_no_mwe_no_ner_7.csv', 'w')\n",
    "begin = time.perf_counter()\n",
    "\n",
    "for i in range(len(test_raw)):\n",
    "    row = test_raw.iloc[i]\n",
    "    prediction = classifier[row.kata].predict(np.array([X_test[i]]))\n",
    "    prediction = label_transformer[row.kata].inverse_transform(prediction)\n",
    "    prediction = prediction[0]\n",
    "    res_file.write('{},{},{}\\n'.format(row.id, row.kata, prediction))\n",
    "    sys.stdout.write(\"\\rProgress: {0:.2f} % | Time elapsed: {1}\".format(\n",
    "    i/len(test_raw)*100, time.perf_counter() - begin\n",
    "    ))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "res_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + MWE Corpus Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from preprocessor import stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 99.99 % | Time elapsed: 31.7843216750006834"
     ]
    }
   ],
   "source": [
    "res_file = open('../semisupervised_wsd_rulebased_mwe_no_ner_2.csv', 'w')\n",
    "begin = time.perf_counter()\n",
    "\n",
    "mwe_corpus = json.load(open('mwe.json'))\n",
    "\n",
    "for i in range(len(test_raw)):\n",
    "    row = test_raw.iloc[i]\n",
    "    prediction = classifier[row.kata].predict(np.array([X_test[i]]))\n",
    "    prediction = label_transformer[row.kata].inverse_transform(prediction)\n",
    "    prediction = str(prediction[0])\n",
    "\n",
    "    # MWE\n",
    "    if row.kata in mwe_corpus.keys():\n",
    "        for mwe in mwe_corpus[row.kata]:\n",
    "            if stemmer.stem(mwe) in row.kalimat:\n",
    "                prediction = prediction[:2] + '0x'\n",
    "                break\n",
    "    \n",
    "    \n",
    "    res_file.write('{},{},{}\\n'.format(row.id, row.kata, prediction))\n",
    "    sys.stdout.write(\"\\rProgress: {0:.2f} % | Time elapsed: {1}\".format(\n",
    "    i/len(test_raw)*100, time.perf_counter() - begin\n",
    "    ))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "res_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe = json.load(open('mwe.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mw'"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'mwe'[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test_raw.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                           2\n",
       "id                                                                  41\n",
       "kata                                                             asing\n",
       "kalimat              warga negara asing atau warga negara makmur ya...\n",
       "pos_tags             NN NN JJ CC NN NN NN SC NN RB NEG CD MD VB NNP...\n",
       "clean                warga negara asing warga negara makmur kepala ...\n",
       "targetpos_clean                                                      2\n",
       "targetpos_ori                                                        2\n",
       "targetpos_pos_tag                                                    2\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier['asing'].predict([X_test[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['5301'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_transformer['asing'].inverse_transform([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   c  d\n",
       "0  1  3\n",
       "1  2  2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'a': [1,2], 'b': [3,2]}).rename(columns={'a':'c', 'b':'d'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
